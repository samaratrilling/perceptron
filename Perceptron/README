README
Samara Trilling sat2160

How to run:
Execute run.bash.

Q4------------------------------
Performance:
Correct:	Total:	Percentage:
2226 		2459 	0.905246034974

Observations:
My tagger consistently tagged some nationality names with initial capitals
(like German and Dutch) as adjectives when they should have been nouns, according
to the key. However, it also consistently tagged some nationalities (like Egyptian
and Libyan) as nouns when they should have been adjectives, according to the
key.
This goes to show that nationalities are inherently ambiguous when it comes to parts
of speech, and more context is needed to determine which part of speech they are.
A phrase like "The German forces" should clearly be tagged DET ADJ NOUN whereas
a phrase like "The German sat down" should clearly be tagged DET NOUN VERB ADV.
Perhaps a feature that logs the previous and next tags would be useful in a case
like this.

Q5------------------------------
Performance:
Correct:	Total:	Percentage:
2293 		2459 	0.932492883286

Observations:
The performance improved significantly with the addition of the suffix feature.

However, the tagger seems to miss a lot of words that contain
dashes, and words that start with an initial capital.
Examples-------------
Initial Capital:
< Applied	NOUN
---
> Applied ADJ
< Libyan	ADJ
---
> Libyan NOUN
< German	NOUN
---
> German ADJ
< Earlier	ADV
---
> Earlier NOUN

Dash:
< World-wide	ADJ
---
> World-wide NOUN
< bargain-hunting	NOUN
---
> bargain-hunting ADJ
< temporary-help	ADJ
---
> temporary-help NOUN
< York-based	ADJ
---
> York-based VERB


Q6------------------------------
For question 6, I decided to implement features that
indicate a) whether or not the word has a dash in it
b) whether or not the word starts with a capital letter, and
c) a prefix feature very similar to the suffix feature
I implemented for question 5.

DASH FEATURE
Performance with q5 features + *just* dash feature:
Correct:	Total:	Percentage:
2296 		2459 	0.933712891419

After implementing this feature, more words with dashes
were tagged as adjectives rather than as nouns.
< dispute-settlement NOUN
> dispute-settlement ADJ
< right-to-work NOUN
> right-to-work ADJ
< lump-sum NOUN
> lump-sum ADJ
< Soft-Sell NOUN
> Soft-Sell ADJ
< pork-barrel NOUN
> pork-barrel ADJ
< shorter-term NOUN
> shorter-term ADJ

CAPITAL FEATURE:
Performance with q5 features and *just* capital feature:
2296 		2459 	0.933712891419
Performance with q5 features + DASH feature + CAPITAL feature:
2301 		2459 	0.935746238308

Interestingly, in the version with the DASH and CAPITAL features,
the tagger seemed to (incorrectly) tag many more initial-capital words
as nouns.
Ex:
< Egyptian	ADJ
---
> Egyptian NOUN
< Libyan	ADJ
---
> Libyan NOUN
< American	ADJ
---
> American NOUN

Others, however, it correctly tagged as adjectives.
< Dutch	NOUN
---
> Dutch ADJ
< South	NOUN
< African	NOUN
---
> South ADJ
> African ADJ

PREFIX FEATURE
The prefix feature is the exact opposite of the suffix feature - 
it generates max three prefixes per word, e.g. complicated ->
c, co, com.
Performance
Correct	Total	Percentage
1491 	2459 	0.606344042294

Clearly this feature was resulting in many words being tagged incorrectly
(which makes sense - just because one word starts with a c and is tagged as
a noun doesn't mean that other words that start with c should be tagged as nouns).

I therefore modified my prefix feature to only take into account prefixes
of length 3 or 4. (e.g. complicated -> com, comp).
This improved the score slightly:
Correct	Total	Percentage
1701 	2459 	0.691744611631
But on the whole, a prefix feature is not a useful indicator and results
in increased numbers of incorrect tagging.

My most successful tagger was one that just used bigram, tag, suffix, initial capital
and dash parameters.
FINAL BEST PERFORMANCE:
Performance with q5 features + DASH feature + CAPITAL feature:
2301 		2459 	0.935746238308



eval_tagger.py - evaluation script 
tagger_decoder.py, tagger_config.py, make_dict.py - implementation of a bigram decoder
tagger_history_generator.py - generates the histories to score
tag_dev.dat, tag_dev.key, tag_train.dat - tagging corpus files
tag.model - pre-trained model file for problem 4
history.scores, example.sent - example history score file and sentence
pipe_servers.py - example of how to call our code from python.
